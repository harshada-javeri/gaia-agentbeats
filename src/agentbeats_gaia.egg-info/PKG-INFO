Metadata-Version: 2.4
Name: agentbeats-gaia
Version: 0.1.0
Summary: GAIA Benchmark implementation on AgentBeats - Evaluating General AI Assistants with multi-step reasoning and tool use
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: a2a-sdk[http-server]>=0.3.8
Requires-Dist: datasets>=3.2.0
Requires-Dist: huggingface-hub>=0.27.0
Requires-Dist: litellm>=1.59.6
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: typer>=0.19.2
Requires-Dist: uvicorn>=0.37.0
Requires-Dist: httpx>=0.28.1
Requires-Dist: duckduckgo-search>=7.3.1
Requires-Dist: pillow>=11.0.0
Requires-Dist: duckdb>=1.0.0
Requires-Dist: sqlalchemy>=2.0.0
Requires-Dist: fastapi>=0.104.0
Requires-Dist: pydantic>=2.0.0

# GAIA Benchmark on AgentBeats

**General AI Assistants Benchmark** - Evaluating AI agents on real-world questions requiring multi-step reasoning, tool use, and web search.

## Overview

This is an **agentified** implementation of the [GAIA Benchmark](https://huggingface.co/gaia-benchmark) on the [AgentBeats](https://agentbeats.org) platform. GAIA proposes 450+ real-world questions that require fundamental abilities such as:

- ğŸ§  Multi-step reasoning
- ğŸ” Web browsing and search
- ğŸ§® Mathematical calculations
- ğŸ“„ Multimodal processing (text, images, documents, audio)
- ğŸ› ï¸ Tool use proficiency

### Why GAIA?

GAIA questions are **conceptually simple for humans yet challenging for most advanced AIs**:
- Human respondents: **92% accuracy**
- GPT-4 with plugins: **15% accuracy**

This benchmark provides a realistic evaluation of general AI assistant capabilities.

## Architecture

**â­ Now following AgentBeats Tutorial Standard!** (Based on `tutorial/scenarios/tau2` and `debate`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Green Agent (GAIA Evaluator)                           â”‚
â”‚  - Inherits from GreenAgent base class                  â”‚
â”‚  - Receives EvalRequest (JSON: participants + config)   â”‚
â”‚  - Uses ToolProvider for A2A communication              â”‚
â”‚  - Loads GAIA tasks from Hugging Face                   â”‚
â”‚  - Sends tasks to executor agent                        â”‚
â”‚  - Scores responses against ground truth                â”‚
â”‚  - Returns structured results via TaskUpdater           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ A2A Protocol (via ToolProvider)
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Purple Agent (GAIA Executor)                           â”‚
â”‚  - Standard AgentExecutor with conversation history     â”‚
â”‚  - Uses tools: web_search, python_calculator            â”‚
â”‚  - Multi-step reasoning with GPT-4o                     â”‚
â”‚  - Returns answers in <answer>...</answer> format       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What's New

âœ… **GreenAgent Pattern**: Uses `agentbeats.green_executor.GreenAgent`
âœ… **ToolProvider**: Clean A2A comms via `agentbeats.tool_provider.ToolProvider`
âœ… **Standardized Format**: `EvalRequest`/`EvalResult` for platform compatibility
âœ… **Separate Dockerfiles**: `Dockerfile.gaia-evaluator` + `Dockerfile.gaia-executor`
âœ… **Local Testing**: `scenario.toml` for easy dev/testing
âœ… **Competition Ready**: Follows AgentX-AgentBeats best practices

## Quick Start

### Prerequisites

1. **OpenAI API Key** - for agent reasoning (GPT-4o)
2. **Hugging Face Token** - for GAIA dataset access
   - Get access at: https://huggingface.co/datasets/gaia-benchmark/GAIA
   - Create token at: https://huggingface.co/settings/tokens

### Option 1: Docker (Recommended for Competition)

```bash
# 1. Set environment variables
cp .env.example .env
# Edit .env with your API keys

# 2. Build and run
docker-compose up --build

# The agents will start on:
# - Green agent: http://localhost:9001
# - Purple agent: http://localhost:9002
```

### Option 2: Local Development

```bash
# 1. Install dependencies
pip install -e .

# Or with uv (faster):
uv sync

# 2. Set environment variables
export OPENAI_API_KEY="your-openai-api-key"
export HF_TOKEN="your-huggingface-token"

# 3. Launch complete evaluation
python main.py launch --level 1 --task-ids "0,1,2" --split validation

# Or run agents separately:
python main.py green    # Terminal 1: Start green agent
python main.py purple   # Terminal 2: Start purple agent
```

## Leaderboard Integration

The system includes a comprehensive leaderboard for tracking benchmark results across agents and teams.

### Features

âœ… **Automatic Submission**: Results submitted automatically after evaluation completes  
âœ… **GitHub Webhook Integration**: Submit via GitHub commits and PRs  
âœ… **REST API**: Query leaderboard, teams, and submission history  
âœ… **Database Persistence**: SQLite (local) or PostgreSQL (production)  
âœ… **Ranking System**: Automatic ranking by accuracy and performance  
âœ… **Audit Trail**: Complete webhook event logging  

### Quick Start

```bash
# 1. Initialize database
python scripts/setup_db.py init

# 2. Start leaderboard API
python -m src.leaderboard_api

# 3. Get leaderboard
curl http://localhost:8000/leaderboard?level=1&split=validation

# 4. Create submission
curl -X POST http://localhost:8000/submissions \
  -H "Content-Type: application/json" \
  -d '{
    "agent_name": "my-agent",
    "agent_version": "1.0.0",
    "team_name": "my-team",
    "level": 1,
    "split": "validation",
    "accuracy": 75.5,
    "correct_tasks": 22,
    "total_tasks": 30,
    "average_time_per_task": 2.3,
    "total_time_seconds": 69.0
  }'
```

### API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/leaderboard` | GET | Get top agents for level/split |
| `/leaderboard/teams` | GET | Get team leaderboard |
| `/submissions/{id}` | GET | Get submission details |
| `/agents/{name}/history` | GET | Get agent submission history |
| `/teams/{name}/history` | GET | Get team submission history |
| `/recent` | GET | Get recent submissions |
| `/stats` | GET | Get overall statistics |
| `/submissions` | POST | Create direct submission |
| `/webhooks/github` | POST | GitHub webhook endpoint |

See [LEADERBOARD_SETUP.md](docs/LEADERBOARD_SETUP.md) for complete documentation.

### GitHub Webhook Submission

Submit results directly from GitHub commits or pull requests:

```bash
git commit -m "Improved agent

{
  \"gaia_submission\": {
    \"agent_name\": \"improved-agent\",
    \"agent_version\": \"2.0.0\",
    \"team_name\": \"my-team\",
    \"level\": 1,
    \"split\": \"validation\",
    \"accuracy\": 85.0,
    \"correct_tasks\": 25,
    \"total_tasks\": 30,
    \"average_time_per_task\": 2.1,
    \"total_time_seconds\": 63.0
  }
}
"

git push
```

See [WEBHOOK_EXAMPLES.md](docs/WEBHOOK_EXAMPLES.md) for detailed examples and CI/CD integration.

### Configuration

Add to `scenario.toml` to enable leaderboard submission:

```toml
[config]
level = 1
split = "validation"
task_indices = [0, 1, 2]

# Leaderboard settings
submit_to_leaderboard = true
agent_name = "my-agent"
agent_version = "1.0.0"
team_name = "my-team"
model_used = "gpt-4o"
```

Or via environment variables:

```bash
export DATABASE_URL="sqlite:///./leaderboard.db"
export GITHUB_WEBHOOK_SECRET="your-webhook-secret"
export LEADERBOARD_API_HOST="0.0.0.0"
export LEADERBOARD_API_PORT="8000"
```


## Usage Examples

### Evaluate Specific Tasks

```bash
# Evaluate first 3 tasks from Level 1 validation set
python main.py launch --level 1 --task-ids "0,1,2" --split validation

# Evaluate single task from Level 2
python main.py launch --level 2 --task-ids "5" --split validation

# Evaluate harder tasks (Level 3)
python main.py launch --level 3 --task-ids "0,1" --split validation
```

### Using the Python API

```python
from src.launcher import launch_evaluation

# Launch evaluation programmatically
await launch_evaluation(
    level=1,
    task_ids=[0, 1, 2, 3, 4],
    split="validation"
)
```

## GAIA Dataset Structure

### Difficulty Levels

- **Level 1**: Solvable by "very good LLMs" (most tasks require 1-2 steps)
- **Level 2**: Moderate complexity (multi-step reasoning required)
- **Level 3**: Strong jump in model capabilities needed (complex tool orchestration)

### Task Fields

Each GAIA task contains:

```json
{
  "task_id": "unique_identifier",
  "Question": "The question to answer",
  "Level": "1, 2, or 3",
  "Final answer": "Ground truth answer",
  "file_name": "optional_file.pdf",
  "file_path": "path/to/file",
  "Annotator Metadata": {...}
}
```

## Purple Agent Tools

The purple agent has access to:

### 1. Web Search (`web_search`)

Search the web using DuckDuckGo for current information.

```python
# Example usage in agent
web_search(query="current GDP of France", max_results=5)
```

### 2. Python Calculator (`python_calculator`)

Evaluate mathematical expressions safely.

```python
# Example usage in agent
python_calculator(expression="sqrt(144) + log(100)")
```

## Evaluation Metrics

The green agent computes:

- **Accuracy**: % of correct answers
- **Total Tasks**: Number of tasks evaluated
- **Correct**: Number of correctly answered tasks
- **Errors**: Number of tasks that failed
- **Average Time**: Mean time per task (seconds)

### Answer Matching

Answers are normalized and compared:
- Exact string match (case-insensitive)
- Numeric match (with tolerance of 0.01)

## Project Structure

```
gaia/
â”œâ”€â”€ .scenario/                    # AgentBeats scenario metadata
â”‚   â”œâ”€â”€ scenario.toml            # Scenario configuration
â”‚   â”œâ”€â”€ green_agent_card.toml    # Green agent card
â”‚   â””â”€â”€ purple_agent_card.toml   # Purple agent card
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ green_agent/             # Evaluation orchestrator
â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â””â”€â”€ gaia_green_agent.toml
â”‚   â”œâ”€â”€ purple_agent/            # Task executor with tools
â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â””â”€â”€ tools.py
â”‚   â”œâ”€â”€ utils/                   # Shared utilities
â”‚   â”‚   â”œâ”€â”€ a2a_helpers.py       # A2A protocol helpers
â”‚   â”‚   â”œâ”€â”€ gaia_loader.py       # Dataset loading
â”‚   â”‚   â””â”€â”€ parse_tags.py        # XML tag parsing
â”‚   â””â”€â”€ launcher.py              # Evaluation coordinator
â”œâ”€â”€ main.py                       # CLI entry point
â”œâ”€â”€ pyproject.toml               # Python dependencies
â”œâ”€â”€ Dockerfile                    # Docker build config
â”œâ”€â”€ docker-compose.yml           # Docker orchestration
â”œâ”€â”€ entrypoint.sh                # Docker entrypoint
â”œâ”€â”€ requirements.txt             # Additional dependencies
â””â”€â”€ README.md                     # This file
```

## Development

### Adding New Tools

1. Define tool function in `src/purple_agent/tools.py`:

```python
def my_new_tool(param: str) -> str:
    """Tool description."""
    # Implementation
    return result
```

2. Add tool definition to `TOOL_DEFINITIONS`:

```python
TOOL_DEFINITIONS.append({
    "type": "function",
    "function": {
        "name": "my_new_tool",
        "description": "What this tool does",
        "parameters": {...}
    }
})
```

3. Update `execute_tool_call()` in `purple_agent/agent.py`

### Customizing Evaluation

Modify `src/green_agent/agent.py`:

- `check_answer_correctness()` - Answer matching logic
- `evaluate_purple_agent_on_task()` - Task evaluation workflow
- `GAIAGreenAgentExecutor.execute()` - Overall orchestration

## Troubleshooting

### HF_TOKEN Error

```
ValueError: HF_TOKEN environment variable required
```

**Solution**: Get dataset access at https://huggingface.co/datasets/gaia-benchmark/GAIA and set your token.

### Web Search Fails

```
{"error": "Search failed: ..."}
```

**Solution**: DuckDuckGo may be rate-limiting. Wait a few seconds between searches.

### Import Errors

```
ModuleNotFoundError: No module named 'a2a'
```

**Solution**: Install dependencies: `pip install -e .` or `uv sync`

## Performance Tips

1. **Use GPT-4o or better** - Smaller models struggle with GAIA tasks
2. **Enable tool use** - Most tasks require web search or calculations
3. **Increase max_iterations** - Complex tasks may need >10 tool calls
4. **Cache dataset** - First load downloads ~100MB, subsequent loads are cached

## Citation

If you use GAIA benchmark, please cite:

```bibtex
@article{gaia2023,
  title={GAIA: a benchmark for General AI Assistants},
  author={Mialon, GrÃ©goire and others},
  journal={arXiv preprint arXiv:2311.12983},
  year={2023}
}
```

## Resources

- **GAIA Benchmark**: https://huggingface.co/datasets/gaia-benchmark/GAIA
- **GAIA Leaderboard**: https://huggingface.co/spaces/gaia-benchmark/leaderboard
- **AgentBeats Platform**: https://agentbeats.org
- **A2A Protocol**: https://github.com/agentbeats/a2a-sdk

## License

This implementation follows AgentBeats licensing. GAIA dataset has its own license terms.

## Contributing

Contributions welcome! Key areas:

- [ ] Add more tools (file reading, image analysis)
- [ ] Improve answer matching logic
- [ ] Support test split evaluation
- [ ] Add multimodal capabilities
- [ ] Optimize for AgentX-AgentBeats competition

---

Built with â¤ï¸ for the AgentX-AgentBeats Competition by Berkeley RDI
